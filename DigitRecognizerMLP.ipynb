{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import time\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing files in pandas dataframe\n",
    "train = pd.read_csv('train.csv',header=0)\n",
    "trainOrig = train\n",
    "test = pd.read_csv('test.csv',header = 0)\n",
    "testOrig = test\n",
    "labels = train['label']\n",
    "labelsOrig = labels\n",
    "train.drop('label',axis=1, inplace =True)\n",
    "#test shape =(28000,784), train shape =(42000,784), labels shape = (42000,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Converting dataframes into numpy arrays\n",
    "train = train.as_matrix()\n",
    "test = test.as_matrix()\n",
    "labels = labels.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAf4AAAB9CAYAAABDPAHnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x160bc3adc18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: [1, 0, 9, 4, 3]\n"
     ]
    }
   ],
   "source": [
    "#Visualization of few random hand written digits\n",
    "fig = plt.figure()\n",
    "randLabels = []\n",
    "for i in range(1,6):\n",
    "    k = random.randrange(0,42000,10)\n",
    "    ax = fig.add_subplot(1,5,i)\n",
    "    ax.imshow(train[k,:].reshape(28,28),'gray')\n",
    "    ax.axis('off')\n",
    "    randLabels.append(labels[k])\n",
    "plt.show()\n",
    "print('Labels: {}'.format(randLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#PreProcessing(Data Compression)\n",
    "pca = PCA()\n",
    "pca.fit(train)\n",
    "VarianceRatio = pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Retained Variance: 0.8508641804646783\n",
      "Number of principal components 58\n"
     ]
    }
   ],
   "source": [
    "#Finding number of principal components\n",
    "#Retaining 85% of total variance \n",
    "TotVar = 0 \n",
    "for idx, Var in enumerate(VarianceRatio):\n",
    "    TotVar += Var \n",
    "    if(TotVar>=0.85): \n",
    "        break \n",
    "print('Total Retained Variance: {}'.format(TotVar)) \n",
    "print('Number of principal components {}'.format(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Again fitting pca for 42 principal components\n",
    "pcaRed = PCA(n_components = 58,whiten= True)\n",
    "pcaRed.fit(train)\n",
    "#Compressing training and testing data using fitted data\n",
    "train = pcaRed.transform(train)\n",
    "test = pcaRed.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cross-validation split\n",
    "(trainX, validX, trainY, validY) = train_test_split(train,labels, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 4.26760378\n",
      "Iteration 2, loss = 0.32227282\n",
      "Iteration 3, loss = 0.29629039\n",
      "Iteration 4, loss = 0.27925470\n",
      "Iteration 5, loss = 0.26250056\n",
      "Iteration 6, loss = 0.24369550\n",
      "Iteration 7, loss = 0.22336271\n",
      "Iteration 8, loss = 0.20652717\n",
      "Iteration 9, loss = 0.18919785\n",
      "Iteration 10, loss = 0.17279717\n",
      "Iteration 11, loss = 0.15958012\n",
      "Iteration 12, loss = 0.14686163\n",
      "Iteration 13, loss = 0.13475862\n",
      "Iteration 14, loss = 0.12491385\n",
      "Iteration 15, loss = 0.11601017\n",
      "Iteration 16, loss = 0.10804458\n",
      "Iteration 17, loss = 0.10036061\n",
      "Iteration 18, loss = 0.09400237\n",
      "Iteration 19, loss = 0.08813785\n",
      "Iteration 20, loss = 0.08262713\n",
      "Iteration 21, loss = 0.07761619\n",
      "Iteration 22, loss = 0.07316524\n",
      "Iteration 23, loss = 0.06903447\n",
      "Iteration 24, loss = 0.06532519\n",
      "Iteration 25, loss = 0.06161470\n",
      "Iteration 26, loss = 0.05824136\n",
      "Iteration 27, loss = 0.05512802\n",
      "Iteration 28, loss = 0.05244449\n",
      "Iteration 29, loss = 0.04988246\n",
      "Iteration 30, loss = 0.04746735\n",
      "Iteration 31, loss = 0.04487906\n",
      "Iteration 32, loss = 0.04303770\n",
      "Iteration 33, loss = 0.04070658\n",
      "Iteration 34, loss = 0.03899955\n",
      "Iteration 35, loss = 0.03700585\n",
      "Iteration 36, loss = 0.03551510\n",
      "Iteration 37, loss = 0.03403838\n",
      "Iteration 38, loss = 0.03242475\n",
      "Iteration 39, loss = 0.03113723\n",
      "Iteration 40, loss = 0.03002237\n",
      "Iteration 41, loss = 0.02872516\n",
      "Iteration 42, loss = 0.02759217\n",
      "Iteration 43, loss = 0.02642162\n",
      "Iteration 44, loss = 0.02553712\n",
      "Iteration 45, loss = 0.02465380\n",
      "Iteration 46, loss = 0.02369308\n",
      "Iteration 47, loss = 0.02303440\n",
      "Iteration 48, loss = 0.02201628\n",
      "Iteration 49, loss = 0.02140143\n",
      "Iteration 50, loss = 0.02066006\n",
      "Iteration 51, loss = 0.02010586\n",
      "Iteration 52, loss = 0.01956681\n",
      "Iteration 53, loss = 0.01884559\n",
      "Iteration 54, loss = 0.01832796\n",
      "Iteration 55, loss = 0.01775121\n",
      "Iteration 56, loss = 0.01737760\n",
      "Iteration 57, loss = 0.01688052\n",
      "Iteration 58, loss = 0.01642661\n",
      "Iteration 59, loss = 0.01606775\n",
      "Iteration 60, loss = 0.01560475\n",
      "Iteration 61, loss = 0.01518994\n",
      "Iteration 62, loss = 0.01483335\n",
      "Iteration 63, loss = 0.01452054\n",
      "Iteration 64, loss = 0.01420401\n",
      "Iteration 65, loss = 0.01384848\n",
      "Iteration 66, loss = 0.01360828\n",
      "Iteration 67, loss = 0.01331297\n",
      "Iteration 68, loss = 0.01316285\n",
      "Iteration 69, loss = 0.01282616\n",
      "Iteration 70, loss = 0.01261146\n",
      "Iteration 71, loss = 0.01234592\n",
      "Iteration 72, loss = 0.01213605\n",
      "Iteration 73, loss = 0.01197626\n",
      "Iteration 74, loss = 0.01178085\n",
      "Iteration 75, loss = 0.01154090\n",
      "Iteration 76, loss = 0.01131827\n",
      "Iteration 77, loss = 0.01115391\n",
      "Iteration 78, loss = 0.01097814\n",
      "Iteration 79, loss = 0.01079505\n",
      "Iteration 80, loss = 0.01065918\n",
      "Iteration 81, loss = 0.01046633\n",
      "Iteration 82, loss = 0.01030867\n",
      "Iteration 83, loss = 0.01022517\n",
      "Iteration 84, loss = 0.01007429\n",
      "Iteration 85, loss = 0.00992634\n",
      "Iteration 86, loss = 0.00981149\n",
      "Iteration 87, loss = 0.00971012\n",
      "Iteration 88, loss = 0.00958689\n",
      "Iteration 89, loss = 0.00948012\n",
      "Iteration 90, loss = 0.00940579\n",
      "Iteration 91, loss = 0.00926371\n",
      "Iteration 92, loss = 0.00916341\n",
      "Iteration 93, loss = 0.00908745\n",
      "Iteration 94, loss = 0.00899476\n",
      "Iteration 95, loss = 0.00893911\n",
      "Iteration 96, loss = 0.00882422\n",
      "Iteration 97, loss = 0.00873188\n",
      "Iteration 98, loss = 0.00865830\n",
      "Iteration 99, loss = 0.00859791\n",
      "Iteration 100, loss = 0.00849424\n",
      "Iteration 101, loss = 0.00842389\n",
      "Iteration 102, loss = 0.00835977\n",
      "Iteration 103, loss = 0.00829435\n",
      "Iteration 104, loss = 0.00822126\n",
      "Iteration 105, loss = 0.00818630\n",
      "Iteration 106, loss = 0.00810841\n",
      "Iteration 107, loss = 0.00805188\n",
      "Iteration 108, loss = 0.00802084\n",
      "Iteration 109, loss = 0.00793996\n",
      "Iteration 110, loss = 0.00790183\n",
      "Iteration 111, loss = 0.00786554\n",
      "Iteration 112, loss = 0.00780016\n",
      "Iteration 113, loss = 0.00773581\n",
      "Iteration 114, loss = 0.00769989\n",
      "Iteration 115, loss = 0.00766191\n",
      "Iteration 116, loss = 0.00762061\n",
      "Iteration 117, loss = 0.00756950\n",
      "Iteration 118, loss = 0.00754074\n",
      "Iteration 119, loss = 0.00748621\n",
      "Iteration 120, loss = 0.00744752\n",
      "Iteration 121, loss = 0.00741036\n",
      "Iteration 122, loss = 0.00737745\n",
      "Iteration 123, loss = 0.00734585\n",
      "Iteration 124, loss = 0.00729754\n",
      "Iteration 125, loss = 0.00727389\n",
      "Iteration 126, loss = 0.00723155\n",
      "Iteration 127, loss = 0.00720193\n",
      "Iteration 128, loss = 0.00716893\n",
      "Iteration 129, loss = 0.00713863\n",
      "Iteration 130, loss = 0.00711187\n",
      "Iteration 131, loss = 0.00707278\n",
      "Iteration 132, loss = 0.00704614\n",
      "Iteration 133, loss = 0.00703054\n",
      "Iteration 134, loss = 0.00699751\n",
      "Iteration 135, loss = 0.00697933\n",
      "Iteration 136, loss = 0.00694791\n",
      "Iteration 137, loss = 0.00692302\n",
      "Iteration 138, loss = 0.00689428\n",
      "Iteration 139, loss = 0.00687227\n",
      "Iteration 140, loss = 0.00684852\n",
      "Iteration 141, loss = 0.00683231\n",
      "Iteration 142, loss = 0.00681003\n",
      "Iteration 143, loss = 0.00679376\n",
      "Iteration 144, loss = 0.00676518\n",
      "Iteration 145, loss = 0.00674358\n",
      "Iteration 146, loss = 0.00671804\n",
      "Iteration 147, loss = 0.00670661\n",
      "Iteration 148, loss = 0.00667682\n",
      "Iteration 149, loss = 0.00667002\n",
      "Iteration 150, loss = 0.00664182\n",
      "Iteration 151, loss = 0.00663234\n",
      "Iteration 152, loss = 0.00661097\n",
      "Iteration 153, loss = 0.00659447\n",
      "Iteration 154, loss = 0.00657490\n",
      "Iteration 155, loss = 0.00656786\n",
      "Iteration 156, loss = 0.00654722\n",
      "Iteration 157, loss = 0.00653180\n",
      "Iteration 158, loss = 0.00651072\n",
      "Iteration 159, loss = 0.00650350\n",
      "Iteration 160, loss = 0.00647892\n",
      "Iteration 161, loss = 0.00646769\n",
      "Iteration 162, loss = 0.00646064\n",
      "Iteration 163, loss = 0.00644229\n",
      "Iteration 164, loss = 0.00642950\n",
      "Iteration 165, loss = 0.00641277\n",
      "Iteration 166, loss = 0.00639977\n",
      "Iteration 167, loss = 0.00638189\n",
      "Iteration 168, loss = 0.00637500\n",
      "Iteration 169, loss = 0.00636217\n",
      "Iteration 170, loss = 0.00635346\n",
      "Iteration 171, loss = 0.00633974\n",
      "Iteration 172, loss = 0.00632902\n",
      "Iteration 173, loss = 0.00631897\n",
      "Iteration 174, loss = 0.00630848\n",
      "Iteration 175, loss = 0.00629416\n",
      "Iteration 176, loss = 0.00628668\n",
      "Iteration 177, loss = 0.00627493\n",
      "Iteration 178, loss = 0.00626254\n",
      "Iteration 179, loss = 0.00625469\n",
      "Iteration 180, loss = 0.00624165\n",
      "Iteration 181, loss = 0.00622937\n",
      "Iteration 182, loss = 0.00622124\n",
      "Iteration 183, loss = 0.00622105\n",
      "Iteration 184, loss = 0.00620384\n",
      "Iteration 185, loss = 0.00619514\n",
      "Iteration 186, loss = 0.00618623\n",
      "Iteration 187, loss = 0.00617516\n",
      "Iteration 188, loss = 0.00617047\n",
      "Iteration 189, loss = 0.00616056\n",
      "Iteration 190, loss = 0.00615327\n",
      "Iteration 191, loss = 0.00614379\n",
      "Iteration 192, loss = 0.00613356\n",
      "Iteration 193, loss = 0.00612721\n",
      "Iteration 194, loss = 0.00611860\n",
      "Iteration 195, loss = 0.00611013\n",
      "Iteration 196, loss = 0.00610495\n",
      "Iteration 197, loss = 0.00609592\n",
      "Iteration 198, loss = 0.00609004\n",
      "Iteration 199, loss = 0.00608074\n",
      "Iteration 200, loss = 0.00607510\n",
      "Iteration 201, loss = 0.00606688\n",
      "Iteration 202, loss = 0.00605960\n",
      "Iteration 203, loss = 0.00605283\n",
      "Iteration 204, loss = 0.00604909\n",
      "Iteration 205, loss = 0.00603896\n",
      "Iteration 206, loss = 0.00603507\n",
      "Iteration 207, loss = 0.00602872\n",
      "Iteration 208, loss = 0.00602046\n",
      "Iteration 209, loss = 0.00601547\n",
      "Iteration 210, loss = 0.00601032\n",
      "Iteration 211, loss = 0.00600097\n",
      "Iteration 212, loss = 0.00599864\n",
      "Iteration 213, loss = 0.00598905\n",
      "Iteration 214, loss = 0.00598498\n",
      "Iteration 215, loss = 0.00598223\n",
      "Iteration 216, loss = 0.00597462\n",
      "Iteration 217, loss = 0.00596799\n",
      "Iteration 218, loss = 0.00596157\n",
      "Iteration 219, loss = 0.00595589\n",
      "Iteration 220, loss = 0.00595295\n",
      "Iteration 221, loss = 0.00594704\n",
      "Iteration 222, loss = 0.00594088\n",
      "Iteration 223, loss = 0.00593544\n",
      "Iteration 224, loss = 0.00592967\n",
      "Iteration 225, loss = 0.00592636\n",
      "Iteration 226, loss = 0.00592343\n",
      "Iteration 227, loss = 0.00591772\n",
      "Iteration 228, loss = 0.00591292\n",
      "Iteration 229, loss = 0.00590899\n",
      "Iteration 230, loss = 0.00590548\n",
      "Iteration 231, loss = 0.00589674\n",
      "Iteration 232, loss = 0.00589436\n",
      "Iteration 233, loss = 0.00588967\n",
      "Iteration 234, loss = 0.00588433\n",
      "Iteration 235, loss = 0.00588307\n",
      "Iteration 236, loss = 0.00587647\n",
      "Iteration 237, loss = 0.00587353\n",
      "Iteration 238, loss = 0.00586914\n",
      "Iteration 239, loss = 0.00586381\n",
      "Iteration 240, loss = 0.00586081\n",
      "Iteration 241, loss = 0.00585700\n",
      "Iteration 242, loss = 0.00585362\n",
      "Iteration 243, loss = 0.00585134\n",
      "Iteration 244, loss = 0.00584447\n",
      "Iteration 245, loss = 0.00584130\n",
      "Iteration 246, loss = 0.00583753\n",
      "Iteration 247, loss = 0.00583255\n",
      "Iteration 248, loss = 0.00583003\n",
      "Iteration 249, loss = 0.00582701\n",
      "Iteration 250, loss = 0.00582332\n",
      "Iteration 251, loss = 0.00582042\n",
      "Iteration 252, loss = 0.00581617\n",
      "Iteration 253, loss = 0.00581530\n",
      "Iteration 254, loss = 0.00580967\n",
      "Iteration 255, loss = 0.00580692\n",
      "Iteration 256, loss = 0.00580308\n",
      "Iteration 257, loss = 0.00580164\n",
      "Iteration 258, loss = 0.00579642\n",
      "Iteration 259, loss = 0.00579611\n",
      "Iteration 260, loss = 0.00578954\n",
      "Iteration 261, loss = 0.00578818\n",
      "Iteration 262, loss = 0.00578548\n",
      "Iteration 263, loss = 0.00578226\n",
      "Iteration 264, loss = 0.00577786\n",
      "Iteration 265, loss = 0.00577485\n",
      "Iteration 266, loss = 0.00577340\n",
      "Iteration 267, loss = 0.00576915\n",
      "Iteration 268, loss = 0.00576820\n",
      "Iteration 269, loss = 0.00576458\n",
      "Iteration 270, loss = 0.00576261\n",
      "Iteration 271, loss = 0.00575948\n",
      "Iteration 272, loss = 0.00575619\n",
      "Iteration 273, loss = 0.00575191\n",
      "Iteration 274, loss = 0.00575163\n",
      "Iteration 275, loss = 0.00574882\n",
      "Iteration 276, loss = 0.00574617\n",
      "Iteration 277, loss = 0.00574213\n",
      "Iteration 278, loss = 0.00573908\n",
      "Iteration 279, loss = 0.00573939\n",
      "Iteration 280, loss = 0.00573464\n",
      "Iteration 281, loss = 0.00573144\n",
      "Iteration 282, loss = 0.00573138\n",
      "Iteration 283, loss = 0.00572681\n",
      "Iteration 284, loss = 0.00572650\n",
      "Iteration 285, loss = 0.00572396\n",
      "Iteration 286, loss = 0.00571966\n",
      "Iteration 287, loss = 0.00571891\n",
      "Iteration 288, loss = 0.00571637\n",
      "Iteration 289, loss = 0.00571333\n",
      "Iteration 290, loss = 0.00571196\n",
      "Iteration 291, loss = 0.00570855\n",
      "Iteration 292, loss = 0.00570627\n",
      "Iteration 293, loss = 0.00570465\n",
      "Iteration 294, loss = 0.00570227\n",
      "Iteration 295, loss = 0.00569959\n",
      "Iteration 296, loss = 0.00569862\n",
      "Iteration 297, loss = 0.00569750\n",
      "Iteration 298, loss = 0.00569409\n",
      "Iteration 299, loss = 0.00569184\n",
      "Iteration 300, loss = 0.00569208\n",
      "Iteration 301, loss = 0.00568899\n",
      "Iteration 302, loss = 0.00568602\n",
      "Iteration 303, loss = 0.00568362\n",
      "Iteration 304, loss = 0.00568228\n",
      "Iteration 305, loss = 0.00567954\n",
      "Iteration 306, loss = 0.00567841\n",
      "Iteration 307, loss = 0.00567642\n",
      "Iteration 308, loss = 0.00567470\n",
      "Iteration 309, loss = 0.00567178\n",
      "Iteration 310, loss = 0.00567115\n",
      "Iteration 311, loss = 0.00566813\n",
      "Iteration 312, loss = 0.00566710\n",
      "Iteration 313, loss = 0.00566659\n",
      "Iteration 314, loss = 0.00566342\n",
      "Iteration 315, loss = 0.00566145\n",
      "Iteration 316, loss = 0.00565946\n",
      "Iteration 317, loss = 0.00565854\n",
      "Iteration 318, loss = 0.00565629\n",
      "Iteration 319, loss = 0.00565419\n",
      "Iteration 320, loss = 0.00565229\n",
      "Iteration 321, loss = 0.00565033\n",
      "Iteration 322, loss = 0.00564996\n",
      "Iteration 323, loss = 0.00564791\n",
      "Iteration 324, loss = 0.00564710\n",
      "Iteration 325, loss = 0.00564398\n",
      "Iteration 326, loss = 0.00564280\n",
      "Iteration 327, loss = 0.00564265\n",
      "Iteration 328, loss = 0.00563888\n",
      "Iteration 329, loss = 0.00563808\n",
      "Iteration 330, loss = 0.00563538\n",
      "Iteration 331, loss = 0.00563551\n",
      "Iteration 332, loss = 0.00563335\n",
      "Iteration 333, loss = 0.00563227\n",
      "Iteration 334, loss = 0.00562935\n",
      "Iteration 335, loss = 0.00563027\n",
      "Iteration 336, loss = 0.00562782\n",
      "Iteration 337, loss = 0.00562633\n",
      "Iteration 338, loss = 0.00562385\n",
      "Iteration 339, loss = 0.00562433\n",
      "Iteration 340, loss = 0.00562122\n",
      "Iteration 341, loss = 0.00562099\n",
      "Iteration 342, loss = 0.00561894\n",
      "Iteration 343, loss = 0.00561766\n",
      "Iteration 344, loss = 0.00561483\n",
      "Iteration 345, loss = 0.00561468\n",
      "Iteration 346, loss = 0.00561372\n",
      "Iteration 347, loss = 0.00561269\n",
      "Iteration 348, loss = 0.00560897\n",
      "Iteration 349, loss = 0.00561003\n",
      "Iteration 350, loss = 0.00560802\n",
      "Iteration 351, loss = 0.00560590\n",
      "Iteration 352, loss = 0.00560475\n",
      "Iteration 353, loss = 0.00560406\n",
      "Iteration 354, loss = 0.00560268\n",
      "Iteration 355, loss = 0.00560107\n",
      "Iteration 356, loss = 0.00559958\n",
      "Iteration 357, loss = 0.00559773\n",
      "Iteration 358, loss = 0.00559738\n",
      "Iteration 359, loss = 0.00559604\n",
      "Iteration 360, loss = 0.00559439\n",
      "Iteration 361, loss = 0.00559436\n",
      "Iteration 362, loss = 0.00559335\n",
      "Iteration 363, loss = 0.00559038\n",
      "Iteration 364, loss = 0.00558933\n",
      "Iteration 365, loss = 0.00558967\n",
      "Iteration 366, loss = 0.00558781\n",
      "Iteration 367, loss = 0.00558619\n",
      "Iteration 368, loss = 0.00558547\n",
      "Iteration 369, loss = 0.00558425\n",
      "Iteration 370, loss = 0.00558190\n",
      "Iteration 371, loss = 0.00558146\n",
      "Iteration 372, loss = 0.00558078\n",
      "Iteration 373, loss = 0.00557905\n",
      "Iteration 374, loss = 0.00557787\n",
      "Iteration 375, loss = 0.00557719\n",
      "Iteration 376, loss = 0.00557582\n",
      "Iteration 377, loss = 0.00557375\n",
      "Iteration 378, loss = 0.00557298\n",
      "Iteration 379, loss = 0.00557203\n",
      "Iteration 380, loss = 0.00557181\n",
      "Iteration 381, loss = 0.00556945\n",
      "Iteration 382, loss = 0.00556978\n",
      "Iteration 383, loss = 0.00556841\n",
      "Iteration 384, loss = 0.00556729\n",
      "Iteration 385, loss = 0.00556632\n",
      "Iteration 386, loss = 0.00556558\n",
      "Iteration 387, loss = 0.00556380\n",
      "Iteration 388, loss = 0.00556256\n",
      "Iteration 389, loss = 0.00556169\n",
      "Iteration 390, loss = 0.00556061\n",
      "Iteration 391, loss = 0.00555957\n",
      "Iteration 392, loss = 0.00555827\n",
      "Iteration 393, loss = 0.00555819\n",
      "Iteration 394, loss = 0.00555769\n",
      "Iteration 395, loss = 0.00555498\n",
      "Iteration 396, loss = 0.00555541\n",
      "Iteration 397, loss = 0.00555390\n",
      "Iteration 398, loss = 0.00555229\n",
      "Iteration 399, loss = 0.00555264\n",
      "Iteration 400, loss = 0.00555078\n",
      "Iteration 401, loss = 0.00554920\n",
      "Iteration 402, loss = 0.00554974\n",
      "Iteration 403, loss = 0.00554879\n",
      "Iteration 404, loss = 0.00554695\n",
      "Iteration 405, loss = 0.00554719\n",
      "Iteration 406, loss = 0.00554535\n",
      "Iteration 407, loss = 0.00554465\n",
      "Iteration 408, loss = 0.00554271\n",
      "Iteration 409, loss = 0.00554314\n",
      "Iteration 410, loss = 0.00554152\n",
      "Iteration 411, loss = 0.00554102\n",
      "Iteration 412, loss = 0.00553934\n",
      "Iteration 413, loss = 0.00553869\n",
      "Iteration 414, loss = 0.00553869\n",
      "Iteration 415, loss = 0.00553768\n",
      "Iteration 416, loss = 0.00553532\n",
      "Iteration 417, loss = 0.00553519\n",
      "Iteration 418, loss = 0.00553365\n",
      "Iteration 419, loss = 0.00553353\n",
      "Iteration 420, loss = 0.00553206\n",
      "Iteration 421, loss = 0.00553103\n",
      "Iteration 422, loss = 0.00553090\n",
      "Iteration 423, loss = 0.00552951\n",
      "Iteration 424, loss = 0.00552941\n",
      "Iteration 425, loss = 0.00552870\n",
      "Iteration 426, loss = 0.00552685\n",
      "Iteration 427, loss = 0.00552655\n",
      "Iteration 428, loss = 0.00552579\n",
      "Iteration 429, loss = 0.00552486\n",
      "Iteration 430, loss = 0.00552400\n",
      "Iteration 431, loss = 0.00552227\n",
      "Iteration 432, loss = 0.00552330\n",
      "Iteration 433, loss = 0.00552128\n",
      "Iteration 434, loss = 0.00552115\n",
      "Iteration 435, loss = 0.00551978\n",
      "Iteration 436, loss = 0.00551874\n",
      "Iteration 437, loss = 0.00551811\n",
      "Iteration 438, loss = 0.00551802\n",
      "Iteration 439, loss = 0.00551678\n",
      "Iteration 440, loss = 0.00551580\n",
      "Iteration 441, loss = 0.00551484\n",
      "Iteration 442, loss = 0.00551430\n",
      "Iteration 443, loss = 0.00551404\n",
      "Iteration 444, loss = 0.00551395\n",
      "Iteration 445, loss = 0.00551218\n",
      "Iteration 446, loss = 0.00551209\n",
      "Iteration 447, loss = 0.00551100\n",
      "Iteration 448, loss = 0.00550979\n",
      "Iteration 449, loss = 0.00550886\n",
      "Iteration 450, loss = 0.00550797\n",
      "Iteration 451, loss = 0.00550785\n",
      "Iteration 452, loss = 0.00550687\n",
      "Iteration 453, loss = 0.00550576\n",
      "Iteration 454, loss = 0.00550495\n",
      "Iteration 455, loss = 0.00550454\n",
      "Iteration 456, loss = 0.00550300\n",
      "Iteration 457, loss = 0.00550296\n",
      "Iteration 458, loss = 0.00550299\n",
      "Iteration 459, loss = 0.00550178\n",
      "Iteration 460, loss = 0.00550077\n",
      "Iteration 461, loss = 0.00550026\n",
      "Iteration 462, loss = 0.00549988\n",
      "Iteration 463, loss = 0.00549880\n",
      "Iteration 464, loss = 0.00549855\n",
      "Iteration 465, loss = 0.00549755\n",
      "Iteration 466, loss = 0.00549668\n",
      "Iteration 467, loss = 0.00549584\n",
      "Iteration 468, loss = 0.00549580\n",
      "Iteration 469, loss = 0.00549416\n",
      "Iteration 470, loss = 0.00549395\n",
      "Iteration 471, loss = 0.00549343\n",
      "Iteration 472, loss = 0.00549244\n",
      "Iteration 473, loss = 0.00549193\n",
      "Iteration 474, loss = 0.00549085\n",
      "Iteration 475, loss = 0.00549016\n",
      "Iteration 476, loss = 0.00549048\n",
      "Iteration 477, loss = 0.00548943\n",
      "Iteration 478, loss = 0.00548801\n",
      "Iteration 479, loss = 0.00548759\n",
      "Iteration 480, loss = 0.00548712\n",
      "Iteration 481, loss = 0.00548616\n",
      "Iteration 482, loss = 0.00548561\n",
      "Iteration 483, loss = 0.00548481\n",
      "Iteration 484, loss = 0.00548445\n",
      "Iteration 485, loss = 0.00548352\n",
      "Iteration 486, loss = 0.00548281\n",
      "Iteration 487, loss = 0.00548177\n",
      "Iteration 488, loss = 0.00548124\n",
      "Iteration 489, loss = 0.00548080\n",
      "Iteration 490, loss = 0.00548082\n",
      "Iteration 491, loss = 0.00547924\n",
      "Iteration 492, loss = 0.00548013\n",
      "Iteration 493, loss = 0.00547848\n",
      "Iteration 494, loss = 0.00547806\n",
      "Iteration 495, loss = 0.00547746\n",
      "Iteration 496, loss = 0.00547654\n",
      "Iteration 497, loss = 0.00547587\n",
      "Iteration 498, loss = 0.00547456\n",
      "Iteration 499, loss = 0.00547444\n",
      "Iteration 500, loss = 0.00547370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', alpha=0.001, batch_size=400, beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(500,), learning_rate='constant',\n",
       "       learning_rate_init=0.2, max_iter=500, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='sgd', tol=1e-08, validation_fraction=0.1,\n",
       "       verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes =(500,) , activation ='logistic' , solver = 'sgd', alpha = 0.001, batch_size = 400, learning_rate ='constant',learning_rate_init = 0.2 , shuffle =True , verbose =True ,max_iter =500,tol = 1e-8)\n",
    "clf.fit(trainX,trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.99      0.98       863\n",
      "          1       0.99      0.99      0.99       922\n",
      "          2       0.97      0.97      0.97       820\n",
      "          3       0.98      0.96      0.97       884\n",
      "          4       0.98      0.98      0.98       806\n",
      "          5       0.97      0.97      0.97       759\n",
      "          6       0.98      0.98      0.98       799\n",
      "          7       0.97      0.98      0.98       889\n",
      "          8       0.96      0.96      0.96       792\n",
      "          9       0.97      0.96      0.96       866\n",
      "\n",
      "avg / total       0.98      0.97      0.97      8400\n",
      "\n",
      "The training accuracy is: 1.0\n",
      "The cross-validation accuracy is: 0.975\n"
     ]
    }
   ],
   "source": [
    "#cross validation report\n",
    "predsValid = clf.predict(validX)\n",
    "predsTrain = clf.predict(trainX)\n",
    "print (classification_report(validY, predsValid))\n",
    "print('The training accuracy is:', accuracy_score(trainY, predsTrain))\n",
    "print ('The cross-validation accuracy is:', accuracy_score(validY, predsValid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdIAAABVCAYAAAD9omXiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDRhFamBgYGBgYANGkRoYGBgYGNjA/wNfeIbmQ5gTgAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x160bc46f7b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Labels: [9, 3, 9, 9, 3, 7, 3, 9, 9]\n",
      "Original Labels: [9, 3, 9, 9, 3, 7, 3, 9, 9]\n"
     ]
    }
   ],
   "source": [
    "#Visualization of few random hand written digits from cross validation set\n",
    "fig = plt.figure()\n",
    "LabelsPred = []\n",
    "LabelsOrig = []\n",
    "for i in range(1,10):\n",
    "    k = random.randrange(0,validX.shape[1],10)\n",
    "    ax = fig.add_subplot(1,10,i)\n",
    "    ax.imshow(pcaRed.inverse_transform(validX[k,:]).reshape(28,28),'gray')\n",
    "    ax.axis('off')\n",
    "    LabelsPred.append(predsValid[k])\n",
    "    LabelsOrig.append(validY[k])\n",
    "plt.show()\n",
    "print('Predicted Labels: {}'.format(LabelsPred))\n",
    "print('Original Labels: {}'.format(LabelsOrig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.45267969\n",
      "Iteration 2, loss = 0.30670177\n",
      "Iteration 3, loss = 0.28365305\n",
      "Iteration 4, loss = 0.26085523\n",
      "Iteration 5, loss = 0.23760028\n",
      "Iteration 6, loss = 0.21546892\n",
      "Iteration 7, loss = 0.19523123\n",
      "Iteration 8, loss = 0.17763891\n",
      "Iteration 9, loss = 0.16119380\n",
      "Iteration 10, loss = 0.14760900\n",
      "Iteration 11, loss = 0.13466610\n",
      "Iteration 12, loss = 0.12488590\n",
      "Iteration 13, loss = 0.11471050\n",
      "Iteration 14, loss = 0.10588865\n",
      "Iteration 15, loss = 0.09824705\n",
      "Iteration 16, loss = 0.09174092\n",
      "Iteration 17, loss = 0.08588568\n",
      "Iteration 18, loss = 0.07983442\n",
      "Iteration 19, loss = 0.07526245\n",
      "Iteration 20, loss = 0.07076038\n",
      "Iteration 21, loss = 0.06640203\n",
      "Iteration 22, loss = 0.06269671\n",
      "Iteration 23, loss = 0.05891458\n",
      "Iteration 24, loss = 0.05594325\n",
      "Iteration 25, loss = 0.05317584\n",
      "Iteration 26, loss = 0.05018322\n",
      "Iteration 27, loss = 0.04761985\n",
      "Iteration 28, loss = 0.04510951\n",
      "Iteration 29, loss = 0.04317040\n",
      "Iteration 30, loss = 0.04082861\n",
      "Iteration 31, loss = 0.03898747\n",
      "Iteration 32, loss = 0.03711255\n",
      "Iteration 33, loss = 0.03543675\n",
      "Iteration 34, loss = 0.03383933\n",
      "Iteration 35, loss = 0.03237164\n",
      "Iteration 36, loss = 0.03091347\n",
      "Iteration 37, loss = 0.02962406\n",
      "Iteration 38, loss = 0.02867589\n",
      "Iteration 39, loss = 0.02724641\n",
      "Iteration 40, loss = 0.02633714\n",
      "Iteration 41, loss = 0.02509525\n",
      "Iteration 42, loss = 0.02421466\n",
      "Iteration 43, loss = 0.02342892\n",
      "Iteration 44, loss = 0.02251893\n",
      "Iteration 45, loss = 0.02176434\n",
      "Iteration 46, loss = 0.02096317\n",
      "Iteration 47, loss = 0.02020349\n",
      "Iteration 48, loss = 0.01965951\n",
      "Iteration 49, loss = 0.01892511\n",
      "Iteration 50, loss = 0.01849714\n",
      "Iteration 51, loss = 0.01798774\n",
      "Iteration 52, loss = 0.01741043\n",
      "Iteration 53, loss = 0.01697329\n",
      "Iteration 54, loss = 0.01641969\n",
      "Iteration 55, loss = 0.01596264\n",
      "Iteration 56, loss = 0.01554020\n",
      "Iteration 57, loss = 0.01519854\n",
      "Iteration 58, loss = 0.01478240\n",
      "Iteration 59, loss = 0.01448738\n",
      "Iteration 60, loss = 0.01416482\n",
      "Iteration 61, loss = 0.01382198\n",
      "Iteration 62, loss = 0.01349330\n",
      "Iteration 63, loss = 0.01327985\n",
      "Iteration 64, loss = 0.01297977\n",
      "Iteration 65, loss = 0.01273137\n",
      "Iteration 66, loss = 0.01249432\n",
      "Iteration 67, loss = 0.01222473\n",
      "Iteration 68, loss = 0.01208703\n",
      "Iteration 69, loss = 0.01186400\n",
      "Iteration 70, loss = 0.01167433\n",
      "Iteration 71, loss = 0.01148252\n",
      "Iteration 72, loss = 0.01129450\n",
      "Iteration 73, loss = 0.01109947\n",
      "Iteration 74, loss = 0.01094276\n",
      "Iteration 75, loss = 0.01077482\n",
      "Iteration 76, loss = 0.01059221\n",
      "Iteration 77, loss = 0.01050318\n",
      "Iteration 78, loss = 0.01031660\n",
      "Iteration 79, loss = 0.01021890\n",
      "Iteration 80, loss = 0.01006233\n",
      "Iteration 81, loss = 0.00997238\n",
      "Iteration 82, loss = 0.00981508\n",
      "Iteration 83, loss = 0.00973486\n",
      "Iteration 84, loss = 0.00962325\n",
      "Iteration 85, loss = 0.00949236\n",
      "Iteration 86, loss = 0.00937982\n",
      "Iteration 87, loss = 0.00932482\n",
      "Iteration 88, loss = 0.00921376\n",
      "Iteration 89, loss = 0.00911982\n",
      "Iteration 90, loss = 0.00905197\n",
      "Iteration 91, loss = 0.00897130\n",
      "Iteration 92, loss = 0.00887403\n",
      "Iteration 93, loss = 0.00881773\n",
      "Iteration 94, loss = 0.00874289\n",
      "Iteration 95, loss = 0.00868036\n",
      "Iteration 96, loss = 0.00862030\n",
      "Iteration 97, loss = 0.00851548\n",
      "Iteration 98, loss = 0.00847925\n",
      "Iteration 99, loss = 0.00840271\n",
      "Iteration 100, loss = 0.00834079\n",
      "Iteration 101, loss = 0.00830633\n",
      "Iteration 102, loss = 0.00823999\n",
      "Iteration 103, loss = 0.00819849\n",
      "Iteration 104, loss = 0.00812716\n",
      "Iteration 105, loss = 0.00807190\n",
      "Iteration 106, loss = 0.00803635\n",
      "Iteration 107, loss = 0.00799264\n",
      "Iteration 108, loss = 0.00794362\n",
      "Iteration 109, loss = 0.00788956\n",
      "Iteration 110, loss = 0.00785859\n",
      "Iteration 111, loss = 0.00781192\n",
      "Iteration 112, loss = 0.00777144\n",
      "Iteration 113, loss = 0.00772181\n",
      "Iteration 114, loss = 0.00769644\n",
      "Iteration 115, loss = 0.00765000\n",
      "Iteration 116, loss = 0.00760491\n",
      "Iteration 117, loss = 0.00758691\n",
      "Iteration 118, loss = 0.00755646\n",
      "Iteration 119, loss = 0.00751421\n",
      "Iteration 120, loss = 0.00748457\n",
      "Iteration 121, loss = 0.00746177\n",
      "Iteration 122, loss = 0.00741725\n",
      "Iteration 123, loss = 0.00740378\n",
      "Iteration 124, loss = 0.00736715\n",
      "Iteration 125, loss = 0.00733864\n",
      "Iteration 126, loss = 0.00732166\n",
      "Iteration 127, loss = 0.00728812\n",
      "Iteration 128, loss = 0.00726502\n",
      "Iteration 129, loss = 0.00722309\n",
      "Iteration 130, loss = 0.00721562\n",
      "Iteration 131, loss = 0.00718477\n",
      "Iteration 132, loss = 0.00715370\n",
      "Iteration 133, loss = 0.00713542\n",
      "Iteration 134, loss = 0.00711226\n",
      "Iteration 135, loss = 0.00709827\n",
      "Iteration 136, loss = 0.00708619\n",
      "Iteration 137, loss = 0.00705497\n",
      "Iteration 138, loss = 0.00703061\n",
      "Iteration 139, loss = 0.00700891\n",
      "Iteration 140, loss = 0.00699898\n",
      "Iteration 141, loss = 0.00697705\n",
      "Iteration 142, loss = 0.00695918\n",
      "Iteration 143, loss = 0.00694142\n",
      "Iteration 144, loss = 0.00693140\n",
      "Iteration 145, loss = 0.00690990\n",
      "Iteration 146, loss = 0.00689151\n",
      "Iteration 147, loss = 0.00687635\n",
      "Iteration 148, loss = 0.00685324\n",
      "Iteration 149, loss = 0.00684017\n",
      "Iteration 150, loss = 0.00682500\n",
      "Iteration 151, loss = 0.00681280\n",
      "Iteration 152, loss = 0.00679477\n",
      "Iteration 153, loss = 0.00678453\n",
      "Iteration 154, loss = 0.00677231\n",
      "Iteration 155, loss = 0.00675578\n",
      "Iteration 156, loss = 0.00673584\n",
      "Iteration 157, loss = 0.00673274\n",
      "Iteration 158, loss = 0.00671955\n",
      "Iteration 159, loss = 0.00670197\n",
      "Iteration 160, loss = 0.00669004\n",
      "Iteration 161, loss = 0.00668192\n",
      "Iteration 162, loss = 0.00666711\n",
      "Iteration 163, loss = 0.00666011\n",
      "Iteration 164, loss = 0.00664640\n",
      "Iteration 165, loss = 0.00662897\n",
      "Iteration 166, loss = 0.00662863\n",
      "Iteration 167, loss = 0.00661207\n",
      "Iteration 168, loss = 0.00660717\n",
      "Iteration 169, loss = 0.00659472\n",
      "Iteration 170, loss = 0.00658263\n",
      "Iteration 171, loss = 0.00657512\n",
      "Iteration 172, loss = 0.00656415\n",
      "Iteration 173, loss = 0.00655193\n",
      "Iteration 174, loss = 0.00654091\n",
      "Iteration 175, loss = 0.00653511\n",
      "Iteration 176, loss = 0.00652539\n",
      "Iteration 177, loss = 0.00651859\n",
      "Iteration 178, loss = 0.00650980\n",
      "Iteration 179, loss = 0.00650112\n",
      "Iteration 180, loss = 0.00649616\n",
      "Iteration 181, loss = 0.00648936\n",
      "Iteration 182, loss = 0.00647724\n",
      "Iteration 183, loss = 0.00647047\n",
      "Iteration 184, loss = 0.00646152\n",
      "Iteration 185, loss = 0.00645204\n",
      "Iteration 186, loss = 0.00644681\n",
      "Iteration 187, loss = 0.00643975\n",
      "Iteration 188, loss = 0.00643132\n",
      "Iteration 189, loss = 0.00642317\n",
      "Iteration 190, loss = 0.00641767\n",
      "Iteration 191, loss = 0.00641336\n",
      "Iteration 192, loss = 0.00640329\n",
      "Iteration 193, loss = 0.00639622\n",
      "Iteration 194, loss = 0.00638945\n",
      "Iteration 195, loss = 0.00638164\n",
      "Iteration 196, loss = 0.00637547\n",
      "Iteration 197, loss = 0.00636867\n",
      "Iteration 198, loss = 0.00636596\n",
      "Iteration 199, loss = 0.00635982\n",
      "Iteration 200, loss = 0.00635402\n",
      "Iteration 201, loss = 0.00634713\n",
      "Iteration 202, loss = 0.00634404\n",
      "Iteration 203, loss = 0.00633708\n",
      "Iteration 204, loss = 0.00633007\n",
      "Iteration 205, loss = 0.00632544\n",
      "Iteration 206, loss = 0.00632051\n",
      "Iteration 207, loss = 0.00631610\n",
      "Iteration 208, loss = 0.00631120\n",
      "Iteration 209, loss = 0.00630463\n",
      "Iteration 210, loss = 0.00630122\n",
      "Iteration 211, loss = 0.00629409\n",
      "Iteration 212, loss = 0.00628804\n",
      "Iteration 213, loss = 0.00628777\n",
      "Iteration 214, loss = 0.00627956\n",
      "Iteration 215, loss = 0.00627652\n",
      "Iteration 216, loss = 0.00626901\n",
      "Iteration 217, loss = 0.00626771\n",
      "Iteration 218, loss = 0.00626246\n",
      "Iteration 219, loss = 0.00625862\n",
      "Iteration 220, loss = 0.00625053\n",
      "Iteration 221, loss = 0.00624814\n",
      "Iteration 222, loss = 0.00624735\n",
      "Iteration 223, loss = 0.00624105\n",
      "Iteration 224, loss = 0.00623580\n",
      "Iteration 225, loss = 0.00623161\n",
      "Iteration 226, loss = 0.00622762\n",
      "Iteration 227, loss = 0.00622310\n",
      "Iteration 228, loss = 0.00621971\n",
      "Iteration 229, loss = 0.00621591\n",
      "Iteration 230, loss = 0.00621424\n",
      "Iteration 231, loss = 0.00621043\n",
      "Iteration 232, loss = 0.00620330\n",
      "Iteration 233, loss = 0.00620344\n",
      "Iteration 234, loss = 0.00619987\n",
      "Iteration 235, loss = 0.00619385\n",
      "Iteration 236, loss = 0.00618949\n",
      "Iteration 237, loss = 0.00618742\n",
      "Iteration 238, loss = 0.00617966\n",
      "Iteration 239, loss = 0.00618066\n",
      "Iteration 240, loss = 0.00617553\n",
      "Iteration 241, loss = 0.00617624\n",
      "Iteration 242, loss = 0.00616881\n",
      "Iteration 243, loss = 0.00616985\n",
      "Iteration 244, loss = 0.00616395\n",
      "Iteration 245, loss = 0.00616069\n",
      "Iteration 246, loss = 0.00615475\n",
      "Iteration 247, loss = 0.00615561\n",
      "Iteration 248, loss = 0.00615131\n",
      "Iteration 249, loss = 0.00615076\n",
      "Iteration 250, loss = 0.00614491\n",
      "Iteration 251, loss = 0.00614334\n",
      "Iteration 252, loss = 0.00613769\n",
      "Iteration 253, loss = 0.00613761\n",
      "Iteration 254, loss = 0.00613293\n",
      "Iteration 255, loss = 0.00613007\n",
      "Iteration 256, loss = 0.00613016\n",
      "Iteration 257, loss = 0.00612578\n",
      "Iteration 258, loss = 0.00612222\n",
      "Iteration 259, loss = 0.00612125\n",
      "Iteration 260, loss = 0.00611684\n",
      "Iteration 261, loss = 0.00611284\n",
      "Iteration 262, loss = 0.00611210\n",
      "Iteration 263, loss = 0.00611092\n",
      "Iteration 264, loss = 0.00610787\n",
      "Iteration 265, loss = 0.00610586\n",
      "Iteration 266, loss = 0.00610057\n",
      "Iteration 267, loss = 0.00610074\n",
      "Iteration 268, loss = 0.00609507\n",
      "Iteration 269, loss = 0.00609720\n",
      "Iteration 270, loss = 0.00609318\n",
      "Iteration 271, loss = 0.00609043\n",
      "Iteration 272, loss = 0.00608758\n",
      "Iteration 273, loss = 0.00608585\n",
      "Iteration 274, loss = 0.00608380\n",
      "Iteration 275, loss = 0.00608067\n",
      "Iteration 276, loss = 0.00607970\n",
      "Iteration 277, loss = 0.00607821\n",
      "Iteration 278, loss = 0.00607396\n",
      "Iteration 279, loss = 0.00607288\n",
      "Iteration 280, loss = 0.00607399\n",
      "Iteration 281, loss = 0.00606697\n",
      "Iteration 282, loss = 0.00606640\n",
      "Iteration 283, loss = 0.00606429\n",
      "Iteration 284, loss = 0.00606249\n",
      "Iteration 285, loss = 0.00605995\n",
      "Iteration 286, loss = 0.00605778\n",
      "Iteration 287, loss = 0.00605716\n",
      "Iteration 288, loss = 0.00605459\n",
      "Iteration 289, loss = 0.00605182\n",
      "Iteration 290, loss = 0.00605196\n",
      "Iteration 291, loss = 0.00604821\n",
      "Iteration 292, loss = 0.00604631\n",
      "Iteration 293, loss = 0.00604525\n",
      "Iteration 294, loss = 0.00604159\n",
      "Iteration 295, loss = 0.00604010\n",
      "Iteration 296, loss = 0.00603881\n",
      "Iteration 297, loss = 0.00603773\n",
      "Iteration 298, loss = 0.00603515\n",
      "Iteration 299, loss = 0.00603415\n",
      "Iteration 300, loss = 0.00603255\n",
      "Iteration 301, loss = 0.00602957\n",
      "Iteration 302, loss = 0.00602905\n",
      "Iteration 303, loss = 0.00602689\n",
      "Iteration 304, loss = 0.00602574\n",
      "Iteration 305, loss = 0.00602311\n",
      "Iteration 306, loss = 0.00602016\n",
      "Iteration 307, loss = 0.00602057\n",
      "Iteration 308, loss = 0.00601720\n",
      "Iteration 309, loss = 0.00601574\n",
      "Iteration 310, loss = 0.00601429\n",
      "Iteration 311, loss = 0.00601308\n",
      "Iteration 312, loss = 0.00601152\n",
      "Iteration 313, loss = 0.00601000\n",
      "Iteration 314, loss = 0.00600783\n",
      "Iteration 315, loss = 0.00600753\n",
      "Iteration 316, loss = 0.00600378\n",
      "Iteration 317, loss = 0.00600305\n",
      "Iteration 318, loss = 0.00600214\n",
      "Iteration 319, loss = 0.00599919\n",
      "Iteration 320, loss = 0.00599856\n",
      "Iteration 321, loss = 0.00599746\n",
      "Iteration 322, loss = 0.00599591\n",
      "Iteration 323, loss = 0.00599300\n",
      "Iteration 324, loss = 0.00599334\n",
      "Iteration 325, loss = 0.00599188\n",
      "Iteration 326, loss = 0.00598884\n",
      "Iteration 327, loss = 0.00598952\n",
      "Iteration 328, loss = 0.00598686\n",
      "Iteration 329, loss = 0.00598560\n",
      "Iteration 330, loss = 0.00598474\n",
      "Iteration 331, loss = 0.00598332\n",
      "Iteration 332, loss = 0.00598118\n",
      "Iteration 333, loss = 0.00597987\n",
      "Iteration 334, loss = 0.00597728\n",
      "Iteration 335, loss = 0.00597749\n",
      "Iteration 336, loss = 0.00597627\n",
      "Iteration 337, loss = 0.00597543\n",
      "Iteration 338, loss = 0.00597445\n",
      "Iteration 339, loss = 0.00597165\n",
      "Iteration 340, loss = 0.00597057\n",
      "Iteration 341, loss = 0.00596974\n",
      "Iteration 342, loss = 0.00596949\n",
      "Iteration 343, loss = 0.00596717\n",
      "Iteration 344, loss = 0.00596486\n",
      "Iteration 345, loss = 0.00596379\n",
      "Iteration 346, loss = 0.00596357\n",
      "Iteration 347, loss = 0.00596238\n",
      "Iteration 348, loss = 0.00596135\n",
      "Iteration 349, loss = 0.00596010\n",
      "Iteration 350, loss = 0.00595750\n",
      "Iteration 351, loss = 0.00595777\n",
      "Iteration 352, loss = 0.00595451\n",
      "Iteration 353, loss = 0.00595487\n",
      "Iteration 354, loss = 0.00595366\n",
      "Iteration 355, loss = 0.00595186\n",
      "Iteration 356, loss = 0.00595134\n",
      "Iteration 357, loss = 0.00595059\n",
      "Iteration 358, loss = 0.00595025\n",
      "Iteration 359, loss = 0.00594883\n",
      "Iteration 360, loss = 0.00594737\n",
      "Iteration 361, loss = 0.00594599\n",
      "Iteration 362, loss = 0.00594353\n",
      "Iteration 363, loss = 0.00594237\n",
      "Iteration 364, loss = 0.00594233\n",
      "Iteration 365, loss = 0.00594126\n",
      "Iteration 366, loss = 0.00594001\n",
      "Iteration 367, loss = 0.00593935\n",
      "Iteration 368, loss = 0.00593842\n",
      "Iteration 369, loss = 0.00593619\n",
      "Iteration 370, loss = 0.00593564\n",
      "Iteration 371, loss = 0.00593402\n",
      "Iteration 372, loss = 0.00593259\n",
      "Iteration 373, loss = 0.00593206\n",
      "Iteration 374, loss = 0.00593116\n",
      "Iteration 375, loss = 0.00592950\n",
      "Iteration 376, loss = 0.00592891\n",
      "Iteration 377, loss = 0.00592793\n",
      "Iteration 378, loss = 0.00592734\n",
      "Iteration 379, loss = 0.00592478\n",
      "Iteration 380, loss = 0.00592490\n",
      "Iteration 381, loss = 0.00592485\n",
      "Iteration 382, loss = 0.00592249\n",
      "Iteration 383, loss = 0.00592171\n",
      "Iteration 384, loss = 0.00592115\n",
      "Iteration 385, loss = 0.00592004\n",
      "Iteration 386, loss = 0.00591854\n",
      "Iteration 387, loss = 0.00591754\n",
      "Iteration 388, loss = 0.00591589\n",
      "Iteration 389, loss = 0.00591677\n",
      "Iteration 390, loss = 0.00591511\n",
      "Iteration 391, loss = 0.00591387\n",
      "Iteration 392, loss = 0.00591293\n",
      "Iteration 393, loss = 0.00591118\n",
      "Iteration 394, loss = 0.00591180\n",
      "Iteration 395, loss = 0.00591054\n",
      "Iteration 396, loss = 0.00590868\n",
      "Iteration 397, loss = 0.00590833\n",
      "Iteration 398, loss = 0.00590634\n",
      "Iteration 399, loss = 0.00590720\n",
      "Iteration 400, loss = 0.00590603\n",
      "Iteration 401, loss = 0.00590418\n",
      "Iteration 402, loss = 0.00590459\n",
      "Iteration 403, loss = 0.00590228\n",
      "Iteration 404, loss = 0.00590250\n",
      "Iteration 405, loss = 0.00590098\n",
      "Iteration 406, loss = 0.00590004\n",
      "Iteration 407, loss = 0.00589916\n",
      "Iteration 408, loss = 0.00589799\n",
      "Iteration 409, loss = 0.00589736\n",
      "Iteration 410, loss = 0.00589604\n",
      "Iteration 411, loss = 0.00589574\n",
      "Iteration 412, loss = 0.00589496\n",
      "Iteration 413, loss = 0.00589322\n",
      "Iteration 414, loss = 0.00589361\n",
      "Iteration 415, loss = 0.00589174\n",
      "Iteration 416, loss = 0.00589176\n",
      "Iteration 417, loss = 0.00589065\n",
      "Iteration 418, loss = 0.00588942\n",
      "Iteration 419, loss = 0.00588914\n",
      "Iteration 420, loss = 0.00588739\n",
      "Iteration 421, loss = 0.00588655\n",
      "Iteration 422, loss = 0.00588547\n",
      "Iteration 423, loss = 0.00588515\n",
      "Iteration 424, loss = 0.00588332\n",
      "Iteration 425, loss = 0.00588394\n",
      "Iteration 426, loss = 0.00588308\n",
      "Iteration 427, loss = 0.00588246\n",
      "Iteration 428, loss = 0.00588071\n",
      "Iteration 429, loss = 0.00588023\n",
      "Iteration 430, loss = 0.00587935\n",
      "Iteration 431, loss = 0.00587852\n",
      "Iteration 432, loss = 0.00587769\n",
      "Iteration 433, loss = 0.00587687\n",
      "Iteration 434, loss = 0.00587633\n",
      "Iteration 435, loss = 0.00587571\n",
      "Iteration 436, loss = 0.00587354\n",
      "Iteration 437, loss = 0.00587478\n",
      "Iteration 438, loss = 0.00587449\n",
      "Iteration 439, loss = 0.00587228\n",
      "Iteration 440, loss = 0.00587091\n",
      "Iteration 441, loss = 0.00587054\n",
      "Iteration 442, loss = 0.00587011\n",
      "Iteration 443, loss = 0.00586912\n",
      "Iteration 444, loss = 0.00586914\n",
      "Iteration 445, loss = 0.00586662\n",
      "Iteration 446, loss = 0.00586677\n",
      "Iteration 447, loss = 0.00586706\n",
      "Iteration 448, loss = 0.00586602\n",
      "Iteration 449, loss = 0.00586380\n",
      "Iteration 450, loss = 0.00586354\n",
      "Iteration 451, loss = 0.00586281\n",
      "Iteration 452, loss = 0.00586203\n",
      "Iteration 453, loss = 0.00586121\n",
      "Iteration 454, loss = 0.00586096\n",
      "Iteration 455, loss = 0.00586011\n",
      "Iteration 456, loss = 0.00585863\n",
      "Iteration 457, loss = 0.00585923\n",
      "Iteration 458, loss = 0.00585744\n",
      "Iteration 459, loss = 0.00585666\n",
      "Iteration 460, loss = 0.00585584\n",
      "Iteration 461, loss = 0.00585509\n",
      "Iteration 462, loss = 0.00585454\n",
      "Iteration 463, loss = 0.00585293\n",
      "Iteration 464, loss = 0.00585395\n",
      "Iteration 465, loss = 0.00585406\n",
      "Iteration 466, loss = 0.00585092\n",
      "Iteration 467, loss = 0.00585132\n",
      "Iteration 468, loss = 0.00585150\n",
      "Iteration 469, loss = 0.00584935\n",
      "Iteration 470, loss = 0.00584878\n",
      "Iteration 471, loss = 0.00584779\n",
      "Iteration 472, loss = 0.00584842\n",
      "Iteration 473, loss = 0.00584673\n",
      "Iteration 474, loss = 0.00584652\n",
      "Iteration 475, loss = 0.00584572\n",
      "Iteration 476, loss = 0.00584547\n",
      "Iteration 477, loss = 0.00584359\n",
      "Iteration 478, loss = 0.00584397\n",
      "Iteration 479, loss = 0.00584220\n",
      "Iteration 480, loss = 0.00584264\n",
      "Iteration 481, loss = 0.00584152\n",
      "Iteration 482, loss = 0.00584112\n",
      "Iteration 483, loss = 0.00583997\n",
      "Iteration 484, loss = 0.00583927\n",
      "Iteration 485, loss = 0.00583845\n",
      "Iteration 486, loss = 0.00583771\n",
      "Iteration 487, loss = 0.00583724\n",
      "Iteration 488, loss = 0.00583674\n",
      "Iteration 489, loss = 0.00583675\n",
      "Iteration 490, loss = 0.00583544\n",
      "Iteration 491, loss = 0.00583449\n",
      "Iteration 492, loss = 0.00583368\n",
      "Iteration 493, loss = 0.00583338\n",
      "Iteration 494, loss = 0.00583214\n",
      "Iteration 495, loss = 0.00583202\n",
      "Iteration 496, loss = 0.00583144\n",
      "Iteration 497, loss = 0.00583071\n",
      "Iteration 498, loss = 0.00582945\n",
      "Iteration 499, loss = 0.00583029\n",
      "Iteration 500, loss = 0.00582849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dbn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-351ee21abe9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;31m#Training classifier on whole set of images\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdbn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdfPred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'sample_submission.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdfPred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dbn' is not defined"
     ]
    }
   ],
   "source": [
    "#using classifier to predict test images on public leader board\n",
    "#Training classifier on whole set of images\n",
    "clf.fit(train,labels)\n",
    "pred = dbn.predict(test)\n",
    "dfPred = pd.read_csv('sample_submission.csv',header=0)\n",
    "dfPred['Label']= pd.DataFrame(pred)\n",
    "dfPred.to_csv('mySubmission.csv', index=False)\n",
    "#98.186% LeaderBoard Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
